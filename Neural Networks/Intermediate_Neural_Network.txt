### Implement a Three Input XOR gate ###

# Imports
import numpy as np

# Define the 8 possibilities of our input X1 - X3
# Output Y1
X = np.array(([0, 0, 0], [0, 0, 1], [0, 1, 0], \
              [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]), dtype=float)

y = np.array(([1], [0], [0], [0], [0],
              [0], [0], [1]), dtype=float)

# Choose a value to Predict
xPredicted = np.array(([0,0,1]), dtype=float)
# Maximum if X input array
X = X/np.amax(X, axis=0)
xPredicted = xPredicted/np.amax(xPredicted, axis=0)

# Set up and save loss results in loss file for graphing
lossFile = open("SumSquaredLossList.csv", "w")

# Build Neural Network class
class neural_Network(object):
    def __init__(self):
        # parameters
        self.inputLayerSize = 3     # X1, X2, X3
        self.outputLayerSize = 1       # Y1
        self.hiddenLayerSize = 4    # Size of Hidden Layer

# Set all weights in network to random values
# Build weights of each layer
        self.W1 = np.random.rand(self.inputLayerSize, self.hiddenLayerSize)
        # 4x1 matrix fir hidden layer to output
        self.W2 = np.random.rand(self.hiddenLayerSize, self.outputLayerSize)

# Implement feed-forward propagation through network
# dot product of X (input) and first set of 3x4 weights
def feedForward(self, X):
    self.z = np.dot(X, self.W1)

    # activation sigmoid
    self.z2 = self.activationSigmoid(self.z)

    # dot product of hidden layer (zs) and second set of 4x1 weights
    self.z3 = np.dot(self.z2, self.W2)

    # final activation function
    o = self.activationSigmoid(self.z3)
    return o

# Add backwardPropagation function:
# impliments real trial and error learning for NN
def backwardPropagate(self, X, y, o):
    # backward prop through network
    # calculate error in input
    self.o_error = y - o
    # apply derivative of activationSigmoid to error
    self.o_delta = self.o_error * self.activationSigmoidPrime(o)
    # z2 error: how much our hidden layer weights contributed to error
    self.z2_error = self.o_delta.dot(self.W2.T)
    # apply derivatives of activationSigmoid to z3 error
    self.z2_delta = self.z2_error * self.activationSigmoidPrime(self.z2)
    # Adjust first, second set weights
    self.W1 += X.T.dot(self.z2_delta)
    self.W2 += self.z2.T.dot(self.o_delta)

# Train netowkr at particular time
# Call backwardPropagation and feedForward each time network is trained
def trainNetwork(self, X, y):
    # feed forward loop
    o = self.feedForward(X)
    # backpropagate values
    self.backwardPropagate(X, y, o)

def activationSigmoid(self, s):
    # activate function
    # simple activationSigmoid curve
    return 1 / (1+np.exp(-s))
def activationSigmoidPrime(self, s):
    # First derivative of activationSigmoid
    # Use some calculus
    return s * (1 - s)

# save the epoch values of the loss function to a file
def saveSumSquaredLossList(self, i, error):
    lossFile.write(str(i) + " , " + str(error.tolist()) + '\n')
def saveWeights(self):
    # save this in order to reproduce work
    np.savetxt("weightsLayer1.txt", self.W1, fmt="%s")
    np.savetxt("weightsLayer2.txt", self.W2, fmt="%s")

# run neural network to predict the output based on weights currently trained
def predictOutput(self):
    print("Predicted XOR output data based on trained weights: ")
    print("Expected (X1-X3): \n" + str(xPredicted))
    print("Output (Y1): \n" + str(self.feedForward(xPredicted)))
myNerualNetwork = neural_Network()
trainedEpochs = 1000
trainingEpochs = 100000

# Main learning loop that crosses all request eras
for i in range(trainingEpochs):
    print("Epoch # " + str(i) + "\n")
    print("Network Input : \n" + str(X))
    print("Expected Output of XOR Gate Neural Network: \n" + str(y))
    print("Actual Output from XOR Gate Neural Network: \n" + str(myNerualNetwork.feedForward(X)))
    # mean sum squared loss
    loss = np.mean(np.square((y - myNerualNetwork.feedForward(X))))
    myNerualNetwork.saveSumSquaredLossList(i, loss)
    print("Sum Squared Loss: \n" + str(loss))
    print("\n")
    myNerualNetwork.trainNetwork(X, y)

    # Save training results for reuse
    myNerualNetwork.saveWeights()
    myNerualNetwork.predictOutput()

'''
output:
Epoch # 999
Network Input :
[[0. 0. 0.]
 [0. 0. 1.]
 [0. 1. 0.]
 [0. 1. 1.]
 [1. 0. 0.]
 [1. 0. 1.]
 [1. 1. 0.]
 [1. 1. 1.]]
Expected Output of XOR Gate Neural Network:
[[1.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [0.]
 [1.]]
Actual Output from XOR Gate Neural Network:
[[0.93419893]
 [0.04425737]
 [0.01636304]
 [0.03906686]
 [0.04377351]
 [0.01744497]
 [0.0391143 ]
 [0.93197489]]
Sum Squared Loss:
0.0020575319565093496
Predicted XOR output data based on trained weights:
Expected (X1-X3):
[0. 0. 1.]
Output (Y1):
[0.04422615]
'''