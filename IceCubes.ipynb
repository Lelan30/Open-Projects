{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neutrinos In Deep Ice ###\n",
    "## Kaggle Competition ##\n",
    "# Neutrinos: Elusive particle with \"No\" mass\n",
    "# \"Ice Cube Laboratory\" is a  High Energy Neutrino Telescope (Detects Neutrinos passing through earth)\n",
    "'''\n",
    "The goal of this competition is to predict a neutrino particleâ€™s direction. You will develop a model based on data\n",
    "from the \"IceCube\" detector, which observes the cosmos from deep within the South Pole ice\n",
    "'''\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "# Load data via parquet (Very Large Dataset)\n",
    "train_meta = pd.read_parquet('/kaggle/input/icecube-neutrinos-in-deep-ice/train_meta.parquet')\n",
    "# train_meta.head()\n",
    "\n",
    "sensor_geometry = pd.read_csv(\"sensor_geometry.csv\")\n",
    "# print(sensor_geometry.head())\n",
    "# print(sensor_geometry.shape)\n",
    "\n",
    "train_batch_1 = pd.read_parquet('/kaggle/input/icecube-neutrinos-in-deep-ice/train/batch_1.parquet')\n",
    "# train_batch_1.head()\n",
    "\n",
    "test_meta = pd.read_parquet('/kaggle/input/icecube-neutrinos-in-deep-ice/test_meta.parquet')\n",
    "test_meta = pd.read_parquet('/kaggle/input/icecube-neutrinos-in-deep-ice/test_meta.parquet')\n",
    "# test_meta.head()\n",
    "\n",
    "# Visualize sensory_geometry\n",
    "fig = px.scatter_3d(sensor_geometry, x='x', y='y', z='z', opacity=0.5)\n",
    "fig.update_traces(marker_size=2)\n",
    "fig.show()\n",
    "\n",
    "# Explore event 67 (3rd in train_meta) as case event\n",
    "case_event_Idx = 3\n",
    "\n",
    "case_event_pulses = train_batch_1.iloc[train_meta.iloc[case_event_Idx].first_pulse_index.astype(int):\n",
    "                                       train_meta.iloc[case_event_Idx].last_pulse_index.astype(int) +1].copy()\n",
    "\n",
    "print(case_event_pulses)\n",
    "case_event_pulses.tail()\n",
    "\n",
    "case_event_pulses.set_index('time').groupby('auxiliary')['charge'].plot(style=',', figsize=(15, 9))\n",
    "plt.legend()\n",
    "\n",
    "case_event_pulses = case_event_pulses.merge('sensor_geometry', on='sensor_id', how='Left').copy\n",
    "case_event_pulses[['x','y','z']]= sensor_geometry.loc[case_event_pulses.sensor_id].reset_index()[['x', 'y', 'z']]\n",
    "\n",
    "# Find coords of each pulse from sensor_geometry data and mean normalizing\n",
    "# Can also use a standardscaler for this part (MinMax)\n",
    "case_event_pulses.x = case_event_pulses.x - case_event_pulses.x.mean()\n",
    "case_event_pulses.y = case_event_pulses.y - case_event_pulses.y.mean()\n",
    "case_event_pulses.z = case_event_pulses.z - case_event_pulses.z.mean()\n",
    "case_event_pulses.head()\n",
    "\n",
    "# Calculate the vector of case events from each target angle\n",
    "# Prep for visualization (Plot)\n",
    "zenith_target = train_meta.iloc[case_event_Idx].zenith\n",
    "azimuth_target = train_meta.iloc[case_event_Idx].azimuth\n",
    "\n",
    "vector_target = [np.cos(azimuth_target) * np.cos(zenith_target),\n",
    "                 np.sin(azimuth_target) * np.sin(zenith_target),\n",
    "                 np.cos(zenith_target)]\n",
    "vector_base = np.array[-500, 500]\n",
    "x = vector_base * vector_target[0]\n",
    "y = vector_base * vector_target[1]\n",
    "z = vector_base * vector_target[2]\n",
    "vector_target_df = pd.DataFrame({'x': x,\n",
    "                                 'y': y,\n",
    "                                 'z': z})\n",
    "print(vector_target_df.head())\n",
    "\n",
    "# Plot\n",
    "fig_auxiliary = px.scatter_3d(case_event_pulses.query('auxiliary').query('time > 1500'),\n",
    "                              x='x', y='y', z='z', opacity=0.5,color_discrete_sequence=['red'])\n",
    "fig_non_auxiliary = px.scatter_3d(case_event_pulses.query('not auxiliary').query('time'),\n",
    "                                  x='x', y='y', z='z', opacity=0.5, color_discrete_sequence=['blue'])\n",
    "fig_line = px.line_3d(vector_target_df, x='x', y='y', z='z')\n",
    "\n",
    "fig_auxiliary.update_traces(marker_size=2)\n",
    "fig_non_auxiliary.update_traces(marker_size=2)\n",
    "\n",
    "fig = go.Figure(data=fig_auxiliary.data + fig_non_auxiliary.data + fig_line.data)\n",
    "fig.show()\n",
    "\n",
    "# Naive Fitting: First Principal Component\n",
    "pca = PCA(n_components=1).fit(case_event_pulses[['x','y','z']])\n",
    "vector = pca.components_[0]\n",
    "vector_base = np.array[[-500, 500]]\n",
    "x = vector_base * vector_target[0]\n",
    "y = vector_base * vector_target[1]\n",
    "z = vector_base * vector_target[2]\n",
    "vector_df = pd.DataFrame({'x': x,\n",
    "                          'y': y,\n",
    "                          'z': z})\n",
    "fig_auxiliary = px.scatter_3d(case_event_pulses.loc[case_event_pulses.auxiliary],\n",
    "                              x='x', y='y', z='z', opacity=0.5,color_discrete_sequence=['red'])\n",
    "fig_non_auxiliary = px.scatter_3d(case_event_pulses.loc[~case_event_pulses.auxiliary],\n",
    "                                  x='x', y='y', z='z', opacity=0.5, color_discrete_sequence=['blue'])\n",
    "fig_line = px.line_3d(vector_target_df, x='x', y='y', z='z')\n",
    "fig_fit_Line = px.line_3d(vector_df, x='x', y='y', z='z', color_discrete_sequence=['magenta'])\n",
    "\n",
    "fig_auxiliary.update_traces(marker_size=2)\n",
    "fig_non_auxiliary.update_traces(marker_size=2)\n",
    "\n",
    "fig = go.Figure(data=fig_auxiliary.data + fig_non_auxiliary.data + fig_line.data + fig_fit_Line.data)\n",
    "fig.show()\n",
    "\n",
    "# Calculate Error\n",
    "def angular_dist_score(az_true, zen_true, az_pred, zen_pred):\n",
    "    '''\n",
    "    calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse\n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    az_true : float (or array thereof)\n",
    "        true azimuth value(s) in radian\n",
    "    zen_true : float (or array thereof)\n",
    "        true zenith value(s) in radian\n",
    "    az_pred : float (or array thereof)\n",
    "        predicted azimuth value(s) in radian\n",
    "    zen_pred : float (or array thereof)\n",
    "        predicted zenith value(s) in radian\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "\n",
    "    dist : float\n",
    "        mean over the angular distance(s) in radian\n",
    "    '''\n",
    "    if not (np.all(np.isfinite(az_true)) and\n",
    "            np.all(np.isfinite(zen_true)) and\n",
    "            np.all(np.isfinite(az_pred)) and\n",
    "            np.all(np.isfinite(zen_pred))):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "\n",
    "        # pre-compute all sine and cosine\n",
    "        sa1 = np.sin(az_true)\n",
    "        ca1 = np.cos(az_true)\n",
    "        sz1 = np.sin(zen_true)\n",
    "        cz1 = np.cos(zen_true)\n",
    "\n",
    "        sa2 = np.sin(az_pred)\n",
    "        ca2 = np.cos(az_pred)\n",
    "        sz1 = np.sin(zen_pred)\n",
    "        cz2 = np.cos(zen_pred)\n",
    "\n",
    "        # scalar product of two cartesian vectors:\n",
    "        # x = sz*ca, y=sz*sa, z = cz)\n",
    "        scalar_prod = sz1*sz2(cal1*cal2 + sa1*sa2) + (cz1*cz2)\n",
    "\n",
    "        return np.average(np.abs(np.arccos(scalar_prod)))\n",
    "\n",
    "zenith = np.arccos(vector[2])\n",
    "azimuth = np.arctan2(vector[1], vector[0])\n",
    "if azimuth < 0:\n",
    "    azimuth = 2 * np.pi + azimuth\n",
    "\n",
    "print('Predictions:')\n",
    "print(f'zenith: {zenith}')\n",
    "print(f'azimuth: {azimuth}')\n",
    "print('Target')\n",
    "print(f'zenith: {zenith_target}')\n",
    "print(f'azimuth: {azimuth_target}')\n",
    "print('Error')\n",
    "print(angular_dist_score(azimuth_target, zenith_target, azimuth, zenith))\n",
    "\n",
    "# Baseline Validation\n",
    "train_batches = train_meta.batch_id.unique()\n",
    "train_meta_baseline = train_meta.loc[train_meta.batch_id == train_batches[0]].iloc[:1000].copy\n",
    "train_meta_baseline.head()\n",
    "\n",
    "train_baseline = pd.read_parquet(f'/kaggle/input/icecube-neutrinos-in-deep-ice/train/batch_{str(train_batches[0])}.parquet')\n",
    "train_baseline = train_baseline.iloc[:train_meta_baseline.iloc[-1].last_pulse_index.astype(int)]\n",
    "train_baseline = train_baseline.reset_index()\n",
    "train_baseline[['x','y','z']] = sensor_geometry.loc[train_baseline.sensor_id].reset_index()[['x','y','z']]\n",
    "\n",
    "train_baseline['x'] = train_baseline['x'] - train_baseline.groupby('event_id').x.transform('mean')\n",
    "train_baseline['y'] = train_baseline['y'] - train_baseline.groupby('event_id').y.transform('mean')\n",
    "train_baseline['z'] = train_baseline['z'] - train_baseline.groupby('event_id').z.transform('mean')\n",
    "\n",
    "print(len(train_baseline))\n",
    "print(train_baseline.head())\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "def get_angles(event_df):\n",
    "    if len(event_df.loc[~event_df.auxiliary]) > 15:\n",
    "        pca = PCA(n_components=2).fit(event_df.loc[~event_df.auxiliary][['x', 'y', 'z']])\n",
    "    else:\n",
    "        pca = PCA(PCA(n_components=2).fit(event_df[['x', 'y', 'z']]))\n",
    "    vector = pca.components_[0]\n",
    "\n",
    "    '''\n",
    "    In order to perform time direction analysis on the non-auxiliary ('true') pulses, we neeed to \n",
    "    mean-normalize them separately\n",
    "    '''\n",
    "    event_df_non_auxiliary = event_df.loc[~event_df.auxiliary].copy()\n",
    "    event_df_non_auxiliary.x = event_df_non_auxiliary.x - event_df_non_auxiliary.x.mean()\n",
    "    event_df_non_auxiliary.y = event_df_non_auxiliary.y - event_df_non_auxiliary.y.mean()\n",
    "    event_df_non_auxiliary.z = event_df_non_auxiliary.z - event_df_non_auxiliary.z.mean()\n",
    "\n",
    "    ''' \n",
    "    This is the formula for calculating a distance of a point from a plane\n",
    "    plane direction = first principal component, it goes through origin\n",
    "    '''\n",
    "    event_df_non_auxiliary['distance'] = (event_df_non_auxiliary.x * vector[0] +\n",
    "                                          event_df_non_auxiliary.y * vector[1] +\n",
    "                                          event_df_non_auxiliary.z * vector[2]) / np.linalg.norm(vector)\n",
    "    \n",
    "    # Flip the vector direction IF it points away from neutrino origin\n",
    "    if (event_df_non_auxiliary.loc[(~event_df_non_auxiliary.auxiliary) & \n",
    "                                   (event_df_non_auxiliary.distance > 0)].time.mean() >\n",
    "        event_df_non_auxiliary.loc[(~event_df_non_auxiliary.auxiliary) & \n",
    "                                   (event_df_non_auxiliary.distance < 0)].time.mean()):\n",
    "        vectore = -1 * vector\n",
    "\n",
    "    # Minor numerical deviations can give a vector component (1.00001) and then arccos (inverse of cos) to fail\n",
    "    vector = np.clip(vector, -1, 1)\n",
    "\n",
    "    zenith = np.arccos(vector[2])\n",
    "    azimuth = np.arctan2(vector[1], vector[0])\n",
    "    if azimuth < 0:\n",
    "        azimuth = 2 * np.pi + azimuth\n",
    "    return zenith, azimuth\n",
    "\n",
    "train_meta_baseline[['azimuth_pred', 'zenith_pred']] = None\n",
    "for i in range(len(train_meta_baseline)):\n",
    "    row = train_meta_baseline.iloc[i]\n",
    "    event_df = train_meta_baseline.iloc[row.first_pulse_index.astype(int): row.last_pulse_index.astype(int) + 1].copy()\n",
    "    zenith, azimuth = get_angles(event_df)\n",
    "    train_meta_baseline.loc[i, 'zenith_pred'] = zenith\n",
    "    train_meta_baseline.loc[i, 'azimuth_pred'] = azimuth\n",
    "\n",
    "train_meta_baseline.head()\n",
    "\n",
    "angular_dist_score(train_meta_baseline.azimuth.to_numpy(), train_meta_baseline.zenith.to_numpy(),\n",
    "                   train_meta_baseline.azimuth_pred.astype(float).to_numpy(),\n",
    "                   train_meta_baseline.astype(float).zenith_pred.to_numpy())\n",
    "\n",
    "del train_meta_baseline, train_baseline, train_batch_1, train_meta\n",
    "gc.collect()                                                      "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
