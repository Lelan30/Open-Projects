import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

#%matplotlib inline
sns.set_style("whitegrid")
plt.style.use("fivethirtyeight")

url = 'https://raw.githubusercontent.com/amankharwal/Website-data/master/heart.csv'
df = pd.read_csv(url)
print(df.head())

#Before training the logistic regression we need to observe and analyze
#the data.

#EXPLORATORY DATA ANALYSIS (EDA)
pd.set_option("display.float", "{:.2f}".format)
df.describe()
df.target.value_counts().plot(kind="bar", color=["salmon", "lightblue"])

print(df.isna().sum())
#No null or na values

categorical_val = []
continous_val = []

for column in df.columns:
    print('==========================')
    print(f"{column} : {df[column].unique()}")
    if len(df[column].unique()) <= 10:
        categorical_val.append(column)
    else:
        continous_val.append(column)

plt.figure(figsize=(15, 15))

for i, column in enumerate(categorical_val, 1):
    plt.subplot(3, 3, i)
    df[df["target"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)
    df[df["target"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)
    plt.legend()
    plt.xlabel(column)
#1) cp(Chest Pain): People with cp, 1, 2, 3 are more likely to have heart disease with people with cp 0
#2) restecg(Resting EKG): People with value of 1 are more likely to have heart disease
#3) exang(exercise induding angina: People with a value of 0 (No ==> angina induced by exercise) have more heart diseas than people with value of 1 (yes ==> angina indiced by exercise)
#4) slope {the slope of the ST segment of peak exercise}: People with a slope value of 2 (Downslopins: signs of an unhealthy heart) are more likely to have heart disease than people with a slope value of 2 slope is 0 (Upsloping: best heart rate with exercise) or 1 (Flatsloping: minimal change (typical healthy heart)).
#5) ca {number of major vessels (0-3) stained by fluoroscopy}: the more blood movement the better, so people with ca equal to 0 are more likely to have heart disease.
#6) thal {thalium stress result}: People with a thal value of 2 (defect corrected: once was a defect but ok now) are more likely to have heart disease.

plt.figure(figsize=(15, 15))

for i, column in enumerate(continous_val, 1):
    plt.subplot(3, 2, i)
    df[df["target"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)
    df[df["target"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)
    plt.legend()
    plt.xlabel(column)
#1) trestbps: resting blood pressure anything above 130-140 is generally of concern
#2) chol: greater than 200 is of concern
#3) thalach: People with a maximum of over 140 more likely to have heart disease
#4) the old peak of exercise-indiced ST depression vs. rest look at heart stress during exercise an unhealthy heart will stress more

#Create another figure
plt.figure(figsize= (10, 8))

#scatter with positive examples
plt.scatter(df.age[df.target == 1],
            df.thalach[df.target == 1],
            c="salmon")
#scatter with negative numbers
plt.scatter(df.age[df.target == 0],
            df.thalach[df.target == 0],
            c="lightblue")

#Add some helpful info
plt.title("Heart Diseases In Function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"]);


#CORRELATION MATRIX
#make correlation matrix a little prettier
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15, 15))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidths=0.5,
                 fmt=".2f",
                 cmap="YlGnBu");
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5)


df.drop('target', axis=1).corrwith(df.target).plot(kind='bar', grid=True, figsize=(12, 8),
                                                   title="Correlation with target")
#fbs and chol are the least correlated with target variable
#all other variables have a significant correlation

#DATA PROCESSING
#After Exploring the dataset, observe that we need to convert some
#categorical variables to dummy variables and scale all values before
#training the machine learning models

#use 'get_dummies' method to create dummy columns

categorical_val.remove('target')
dataset = pd.get_dummies(df, columns = categorical_val)

from sklearn.preprocessing import StandardScaler

s_sc = StandardScaler()
col_to_scale = ['age', 'trestbps','chol','thalach', 'oldpeak']
dataset[col_to_scale] = s_sc.fit_transform(dataset[col_to_scale])

#APPLYING LOGISTIC REGRESSION





